seed: 1
epoch: 10
block_size: 20
block_decay: .5
model: lib.model.LeNetMnist
optimizers: ['ksgld.KSGLD','eksgld.EKSGLD','optim.SGD','sgld3.SGLD','psgld2.pSGLD','asgld.ASGLD']
eksgld.EKSGLD:
  lr: 5.e-3
  train_size: 60000
  num_burn_in_steps: 600
  add_noise: True
  eps: 1.e-3   # Tikhonov regularization
  sua: False  # Applies SUA approximation.
  ra: True
  update_freq: 50
  alpha: .5   # Running average parameter (if == 1, no r. ave.).
  update_style: ksgld
  _accept_model: True
ksgld.KSGLD:
  lr: 3.2e-2
  train_size: 60000
  num_burn_in_steps: 600
  add_noise: True
  eps: 1.e-3   # Tikhonov regularization
  sua: True  # Applies SUA approximation.
  pi: False   # pi conrrection for Tikhonov regularization
  update_freq: 50
  alpha: 1.   # Running average parameter (if == 1, no r. ave.).
  constraint_norm: True  # Scale the gradients by the squared fisher norm.
  _accept_model: True
optim.SGD:
  lr: 1.e-1
optim.RMSprop:
  lr: 5.e-4
  weight_decay: 1.e-2
sgld3.SGLD:
  lr: .2
  train_size: 60000
  lr_decay: 2.e-2
  weight_decay: 0.
  num_burn_in_steps: 300
psgld2.pSGLD:
  lr: 1.e-3
  train_size: 60000
  alpha: .95
  eps: 5.e-5
  num_burn_in_steps: 300
  centered: False
  addnoise: True
asgld.ASGLD:
  lr: 1.e-1
  momentum: .9
  weight_decay: 5.e-4
  eps: 1.e-6
  noise: 1.e-2
dataset:
  name: MNIST
  train_batch: 200
  test_batch: 200
# notes:
# train size CIFAR10: 50k
# train size MNIST: 60k