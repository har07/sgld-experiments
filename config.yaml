seed: 1
epoch: 50
block_size: 20
block_decay: .5
preconditioner: None
optimizer: ksgld.KSGLD
accept_model: True
model: resnet.ResNet34
ekfac.EKFAC:
  eps: 1.e-3
  sua: False
  ra: True
  update_freq: 50
  alpha: .5
eksgld.EKSGLD:
  lr: 5.e-3
  train_size: 50000
  num_burn_in_steps: 600
  add_noise: True
  eps: 1.e-3   # Tikhonov regularization
  sua: False  # Applies SUA approximation.
  ra: True
  update_freq: 50
  alpha: .5   # Running average parameter (if == 1, no r. ave.).
  update_style: ksgld
ksgld.KSGLD:
  lr: 3.2e-2
  train_size: 50000
  num_burn_in_steps: 600
  add_noise: True
  eps: 1.e-3   # Tikhonov regularization
  sua: True  # Applies SUA approximation.
  pi: False   # pi conrrection for Tikhonov regularization
  update_freq: 50
  alpha: 1.   # Running average parameter (if == 1, no r. ave.).
  constraint_norm: True  # Scale the gradients by the squared fisher norm.
kfac.KFAC:
  eps: 1.e-3   # Tikhonov regularization
  sua: True  # Applies SUA approximation.
  pi: False   # pi conrrection for Tikhonov regularization
  update_freq: 50
  alpha: 1.   # Running average parameter (if == 1, no r. ave.).
  constraint_norm: False  # Scale the gradients by the squared fisher norm.
optim.SGD:
  lr: 1.e-1
optim.RMSprop:
  lr: 5.e-4
  weight_decay: 1.e-2
pysgmcmc_sgld.SGLD:
  lr: 5.e-4 # for pSGLD
  precondition_decay_rate: .95 # Exponential decay rate of the rescaling of the preconditioner (RMSprop)
  num_burn_in_steps: 10
  diagonal_bias: 1.e-8
sgld3.SGLD:
  lr: .2
  train_size: 50000
  lr_decay: 2.e-2
  weight_decay: 0.
  num_burn_in_steps: 300
psgld2.pSGLD:
  lr: 1.e-3
  train_size: 50000
  alpha: .95
  eps: 5.e-5
  num_burn_in_steps: 300
  centered: False
  addnoise: True
asgld.ASGLD:
  lr: 1.e-1
  momentum: .9
  weight_decay: 5.e-4
  eps: 1.e-6
  noise: 1.e-2
dataset:
  name: CIFAR10
  train_batch: 200
  test_batch: 200
percentage_tosample: .1
step_samples: 10
step_save_state: 100
# notes:
# train size CIFAR10: 50k
# train size MNIST: 60k